{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzouC3wpL6i4Jyyb83b2hx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peeyushsinghal/EVA8/blob/main/S10-Assignment-Solution/EVA8_S10_ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchinfo\n",
        "# !pip install einops"
      ],
      "metadata": {
        "id": "0SvNUruK5O-2"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "Xg9uBPYatmm6"
      },
      "outputs": [],
      "source": [
        "#@title Importing Libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange"
      ],
      "metadata": {
        "id": "4DMmwYQ46_T0"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pair(data):\n",
        "  return data if isinstance(data,tuple) else (data,data)"
      ],
      "metadata": {
        "id": "DvJXEFHaBnOA"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ToPatch(nn.Module):\n",
        "  def __init__(self,  patch_size,  channels=3, embedding_dim = 768):\n",
        "    super().__init__()\n",
        "\n",
        "    self.patch= nn.Sequential(\n",
        "        nn.Conv2d(in_channels = channels, out_channels = embedding_dim, kernel_size = patch_size, stride = patch_size, padding =0), #3X32x32 -> out_channels x (image_height // patch_height) x (image_width // patch_width) [(out_channels)x(image_height // patch_height) x (image_width // patch_width)]\n",
        "        nn.Flatten(start_dim=2, end_dim=3), # conversion to 2d- out_channels x [(image_height // patch_height) x (image_width // patch_width)] == out_channels x num_patches\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.patch(x)\n",
        "    x = x.permute(0,2,1) # [B x out_channels x num_patches] -> [B x  num_patches x out_channels]\n",
        "    return x"
      ],
      "metadata": {
        "id": "cLmtAk-nEijn"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,\n",
        "               in_dim = 32,\n",
        "               out_dim = 3*32,\n",
        "               drop_out = 0.1\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.ff = nn.Sequential(\n",
        "        nn.Conv1d(in_channels=in_dim, out_channels=out_dim, kernel_size = 1), # using 1x1 conv instead of linear layer\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(drop_out),\n",
        "        nn.Conv1d(in_channels=out_dim, out_channels=in_dim, kernel_size = 1), # using 1x1 conv instead of linear layer\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.ff(x)"
      ],
      "metadata": {
        "id": "mdLaQqZJUeYH"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "  def __init__(self,  \n",
        "               num_heads = 4, # number of parallel multi attention heads required\n",
        "               dim = 32, # number of total dimension of input\n",
        "               transformer_dropout = 0.1 #Dropout used in feedforward layer\n",
        "               ):\n",
        "    super().__init__()\n",
        "    # attention block\n",
        "    self.layer_norm_preattn= nn.LayerNorm(dim)\n",
        "    self.self_attn = nn.MultiheadAttention(embed_dim = dim, \n",
        "                                           num_heads = num_heads,\n",
        "                                           batch_first = True)\n",
        "    \n",
        "\n",
        "    # mlp block\n",
        "    self.layer_norm_preff = nn.LayerNorm(dim)\n",
        "    self.feed_forward = FeedForward(in_dim = dim, out_dim = 3*dim, drop_out = transformer_dropout)\n",
        "\n",
        "  \n",
        "\n",
        "  def forward(self,x):\n",
        "    # attention block\n",
        "    x_attn_residual = self.layer_norm_preattn(x)\n",
        "    # print(\"after layer norm, size : \",x_attn_residual.shape)\n",
        "    x_attn_residual, attn_output_weights  = self.self_attn(query =x_attn_residual, \n",
        "                                            key = x_attn_residual, \n",
        "                                            value = x_attn_residual,\n",
        "                                            need_weights = True,\n",
        "                                            average_attn_weights=True)\n",
        "    # print(\"after attention, size : \",x_attn_residual.shape)\n",
        "    x = x + x_attn_residual\n",
        "    # print(\"after residual addition, size : \", x.shape) # batch, (num_patches + 1), (embedding_dim)\n",
        "\n",
        "    # Feed Forward block\n",
        "    x_ff_residual = self.layer_norm_preff(x)\n",
        "    x_ff_residual = self.feed_forward(x_ff_residual.permute(0,2,1)) # permutation required to get B x C x (num_patches + 1) format. C = Embedding Dim\n",
        "    # print(\"after feed forward, size : \", x_ff_residual.shape) \n",
        "    x = x + x_ff_residual.permute(0,2,1) # residual requires permutation to get back into format of x, i.e., B x(num_patches + 1) x C format. C = Embedding Dim\n",
        "    # print(\"after adding residual in feed forward, size : \", x_ff_residual.shape) \n",
        "    return x"
      ],
      "metadata": {
        "id": "N-7FeKr_XO0x"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerStack(nn.Module):\n",
        "  ## MultiHead Attention Block\n",
        "  def __init__(self,\n",
        "               num_blocks = 4, # number of transformers blocks stacked on each other\n",
        "               num_heads = 4, # number of parallel multi attention heads required\n",
        "               dim = 32, # number of total dimension of input\n",
        "               transformer_dropout = 0.1 #Dropout used in attention\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.tranformer_stack = nn.ModuleList([]) # initialized\n",
        "    for _ in range(num_blocks):\n",
        "      self.tranformer_stack.append(TransformerEncoderBlock(num_heads=num_heads, dim = dim, transformer_dropout = transformer_dropout))\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    for transformer_block in self.tranformer_stack:\n",
        "      x = transformer_block(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Mh8dIJBcQbUl"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self,\n",
        "               num_classes = 10, # number of classes\n",
        "               dim = 32, # input dimension\n",
        "               head_p_drop = 0.1 # drop out\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.layer_norm_prehead= nn.LayerNorm(dim)\n",
        "    self.head = nn.Sequential(\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(head_p_drop),\n",
        "        nn.Conv1d(in_channels = dim, out_channels = num_classes, kernel_size = 1)\n",
        "    )\n",
        "\n",
        "  def forward(self,x, pool = 'cls'):\n",
        "    print(\"before head block, size :\", x.shape)\n",
        "    if pool == 'cls':\n",
        "      x_cls = x[:,0,:] # getting the first dimension this gives [batch x dim]\n",
        "    else:\n",
        "      x_cls = x[:,1:,:].mean(dim=1) # ignoring the first dimension  this gives [batch x num_patch x dim], mean gives [batch x dim]\n",
        "    x_cls = x_cls.unsqueeze(dim=1) # [batch x dim] -> batch x 1 x dim\n",
        "    # print(\"before head block, before permutation, size :\", x_cls.shape)\n",
        "    x_cls = self.layer_norm_prehead(x_cls)\n",
        "    x_cls = x_cls.permute(0,2,1)\n",
        "    print(\"before head block, after permutation, size :\", x_cls.shape)\n",
        "    x_cls = self.head(x_cls)\n",
        "    print(\"after head block , size :\", x_cls.shape)\n",
        "    output = x_cls.view(-1,10)\n",
        "    \n",
        "    # if pool == 'mean':\n",
        "    #   x_mean = x[:,1:,:] # ignoring the first dimension  this gives [batch x num_patch x dim]\n",
        "    #   print(\"before taking mean, mean, size :\", x_mean.shape)\n",
        "    #   x_mean = x_mean.mean(dim =1) # this gives [batch x dim]\n",
        "    #   x_mean = x_mean.unsqueeze(dim=1) # [batch x dim] -> batch x 1 x dim\n",
        "    #   print(\"before head block, mean, size :\", x_mean.shape)\n",
        "    #   x_mean = self.layer_norm_prehead(x_mean)\n",
        "    #   print(\"before head block, after layernorm, mean, size :\", x_mean.shape)\n",
        "    #   x_mean = x_mean.permute(0,2,1)\n",
        "    #   print(\"before head block, after permutation, mean, size :\", x_mean.shape)\n",
        "    #   x_mean = self.head(x_mean)\n",
        "    #   print(\"after head block, mean, size :\", x_mean.shape)\n",
        "    #   output = x_mean.view(-1,10)\n",
        "    return output"
      ],
      "metadata": {
        "id": "jwH4Iy1kNqtf"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "  def __init__(self, \n",
        "               image_size, \n",
        "               patch_size, \n",
        "               dim = None, # if None, use the information as per image size else use the dimensions provided\n",
        "               pool = 'cls', # whether the pooling is based on class token ('cls') or mean pooling ('mean')\n",
        "               num_classes = 10, \n",
        "               emb_dropout = 0.1 #Dropout for patch and position embeddings\n",
        "               ):\n",
        "    super().__init__()\n",
        "    \n",
        "    assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "    self.pool = pool\n",
        "\n",
        "    image_height, image_width = pair(image_size)\n",
        "    patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "    assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "    num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "    channels = 3 # hard coding\n",
        "    patch_dim = channels * patch_height * patch_width\n",
        "\n",
        "    if dim:\n",
        "      embedding_dim = dim # specific dimension\n",
        "    else:\n",
        "      embedding_dim = patch_dim # 3xP_hxP_w\n",
        "\n",
        "    self.to_patch = ToPatch(patch_size = patch_size, channels = channels, embedding_dim = embedding_dim)\n",
        "    self.class_token = nn.Parameter(data= torch.randn(1, 1, embedding_dim), requires_grad=True)\n",
        "    self.pos_embedding = nn.Parameter(data = torch.randn(1, num_patches + 1, embedding_dim),requires_grad=True)\n",
        "    self.embedding_dropout = nn.Dropout(p=emb_dropout)\n",
        "\n",
        "    self.transformer = TransformerStack(num_blocks = 4,  num_heads = 4,  dim = 32,  transformer_dropout = 0.1)\n",
        "\n",
        "    self.head_output = Head(num_classes = num_classes, dim = embedding_dim, head_p_drop = 0.1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.to_patch(x)\n",
        "    # print(\"after to_patch, size :\", x.shape)\n",
        "    \n",
        "    batch_size, num_patches, embedding_dim = x.shape[0], x.shape[-2], x.shape[-1]\n",
        "    # print(f'num_patches : {num_patches}, embedding_dim : {embedding_dim}')\n",
        "    \n",
        "    class_token_across_batch = self.class_token.expand(batch_size,-1,-1) # -1 means not to expand in that direction\n",
        "    x = torch.cat((class_token_across_batch,x),dim=1) # dim 0 is batch_size, dim 1 is num_patches and dim 2 is embedding_dim\n",
        "    # print(\"after concatenation with class token, size :\", x.shape)\n",
        "    \n",
        "    pos_emeddings_across_batch = self.pos_embedding.expand(batch_size,-1,-1) # -1 means not to expand in that direction\n",
        "    x = x + pos_emeddings_across_batch \n",
        "    # print(\"after adding with postional embeddings, size :\", x.shape)\n",
        "\n",
        "    x = self.embedding_dropout(x)\n",
        "\n",
        "    x = self.transformer(x)\n",
        "    # print(\"after transformer, size :\", x.shape)\n",
        "\n",
        "    if self.pool == 'cls':\n",
        "      x = self.head_output(x,pool='cls')\n",
        "    if self.pool == 'mean':\n",
        "      x = self.head_output(x,pool='mean')\n",
        "\n",
        "    \n",
        "    print(\"after head_output, size :\", x.shape)\n",
        "    return x"
      ],
      "metadata": {
        "id": "vXt0NBxE5XLH"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class xx(nn.Module):\n",
        "#   def __init__(self,):\n",
        "#     super().__init__()\n",
        "\n",
        "#   def forward(self,x):\n",
        "#     return x"
      ],
      "metadata": {
        "id": "OIQhuEyu7E1a"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"device:\", device)\n",
        "batch_size = 8\n",
        "model = ViT(image_size = 32, patch_size = 2, dim = 32, pool = 'mean').to(device)\n",
        "summary(model, input_size=(batch_size, 3, 32, 32), col_names = ['input_size','output_size','num_params'], depth = 10, row_settings = [\"var_names\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNQSTxbhCRC3",
        "outputId": "a53edd16-0b36-4dfb-bde2-0c2c75953ea5"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cpu\n",
            "before head block, size : torch.Size([8, 257, 32])\n",
            "before head block, before permutation cls, size : torch.Size([8, 1, 32])\n",
            "before head block, after permutation cls, size : torch.Size([8, 32, 1])\n",
            "after head block cls, size : torch.Size([8, 10, 1])\n",
            "after head_output, size : torch.Size([8, 10])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==================================================================================================================================\n",
              "Layer (type (var_name))                                 Input Shape               Output Shape              Param #\n",
              "==================================================================================================================================\n",
              "ViT (ViT)                                               [8, 3, 32, 32]            [8, 10]                   8,256\n",
              "├─ToPatch (to_patch)                                    [8, 3, 32, 32]            [8, 256, 32]              --\n",
              "│    └─Sequential (patch)                               [8, 3, 32, 32]            [8, 32, 256]              --\n",
              "│    │    └─Conv2d (0)                                  [8, 3, 32, 32]            [8, 32, 16, 16]           416\n",
              "│    │    └─Flatten (1)                                 [8, 32, 16, 16]           [8, 32, 256]              --\n",
              "├─Dropout (embedding_dropout)                           [8, 257, 32]              [8, 257, 32]              --\n",
              "├─TransformerStack (transformer)                        [8, 257, 32]              [8, 257, 32]              --\n",
              "│    └─ModuleList (tranformer_stack)                    --                        --                        --\n",
              "│    │    └─TransformerEncoderBlock (0)                 [8, 257, 32]              [8, 257, 32]              --\n",
              "│    │    │    └─LayerNorm (layer_norm_preattn)         [8, 257, 32]              [8, 257, 32]              64\n",
              "│    │    │    └─MultiheadAttention (self_attn)         --                        [8, 257, 32]              4,224\n",
              "│    │    │    └─LayerNorm (layer_norm_preff)           [8, 257, 32]              [8, 257, 32]              64\n",
              "│    │    │    └─FeedForward (feed_forward)             [8, 32, 257]              [8, 32, 257]              --\n",
              "│    │    │    │    └─Sequential (ff)                   [8, 32, 257]              [8, 32, 257]              --\n",
              "│    │    │    │    │    └─Conv1d (0)                   [8, 32, 257]              [8, 96, 257]              3,168\n",
              "│    │    │    │    │    └─GELU (1)                     [8, 96, 257]              [8, 96, 257]              --\n",
              "│    │    │    │    │    └─Dropout (2)                  [8, 96, 257]              [8, 96, 257]              --\n",
              "│    │    │    │    │    └─Conv1d (3)                   [8, 96, 257]              [8, 32, 257]              3,104\n",
              "│    │    └─TransformerEncoderBlock (1)                 [8, 257, 32]              [8, 257, 32]              --\n",
              "│    │    │    └─LayerNorm (layer_norm_preattn)         [8, 257, 32]              [8, 257, 32]              64\n",
              "│    │    │    └─MultiheadAttention (self_attn)         --                        [8, 257, 32]              4,224\n",
              "│    │    │    └─LayerNorm (layer_norm_preff)           [8, 257, 32]              [8, 257, 32]              64\n",
              "│    │    │    └─FeedForward (feed_forward)             [8, 32, 257]              [8, 32, 257]              --\n",
              "│    │    │    │    └─Sequential (ff)                   [8, 32, 257]              [8, 32, 257]              --\n",
              "│    │    │    │    │    └─Conv1d (0)                   [8, 32, 257]              [8, 96, 257]              3,168\n",
              "│    │    │    │    │    └─GELU (1)                     [8, 96, 257]              [8, 96, 257]              --\n",
              "│    │    │    │    │    └─Dropout (2)                  [8, 96, 257]              [8, 96, 257]              --\n",
              "│    │    │    │    │    └─Conv1d (3)                   [8, 96, 257]              [8, 32, 257]              3,104\n",
              "│    │    └─TransformerEncoderBlock (2)                 [8, 257, 32]              [8, 257, 32]              --\n",
              "│    │    │    └─LayerNorm (layer_norm_preattn)         [8, 257, 32]              [8, 257, 32]              64\n",
              "│    │    │    └─MultiheadAttention (self_attn)         --                        [8, 257, 32]              4,224\n",
              "│    │    │    └─LayerNorm (layer_norm_preff)           [8, 257, 32]              [8, 257, 32]              64\n",
              "│    │    │    └─FeedForward (feed_forward)             [8, 32, 257]              [8, 32, 257]              --\n",
              "│    │    │    │    └─Sequential (ff)                   [8, 32, 257]              [8, 32, 257]              --\n",
              "│    │    │    │    │    └─Conv1d (0)                   [8, 32, 257]              [8, 96, 257]              3,168\n",
              "│    │    │    │    │    └─GELU (1)                     [8, 96, 257]              [8, 96, 257]              --\n",
              "│    │    │    │    │    └─Dropout (2)                  [8, 96, 257]              [8, 96, 257]              --\n",
              "│    │    │    │    │    └─Conv1d (3)                   [8, 96, 257]              [8, 32, 257]              3,104\n",
              "│    │    └─TransformerEncoderBlock (3)                 [8, 257, 32]              [8, 257, 32]              --\n",
              "│    │    │    └─LayerNorm (layer_norm_preattn)         [8, 257, 32]              [8, 257, 32]              64\n",
              "│    │    │    └─MultiheadAttention (self_attn)         --                        [8, 257, 32]              4,224\n",
              "│    │    │    └─LayerNorm (layer_norm_preff)           [8, 257, 32]              [8, 257, 32]              64\n",
              "│    │    │    └─FeedForward (feed_forward)             [8, 32, 257]              [8, 32, 257]              --\n",
              "│    │    │    │    └─Sequential (ff)                   [8, 32, 257]              [8, 32, 257]              --\n",
              "│    │    │    │    │    └─Conv1d (0)                   [8, 32, 257]              [8, 96, 257]              3,168\n",
              "│    │    │    │    │    └─GELU (1)                     [8, 96, 257]              [8, 96, 257]              --\n",
              "│    │    │    │    │    └─Dropout (2)                  [8, 96, 257]              [8, 96, 257]              --\n",
              "│    │    │    │    │    └─Conv1d (3)                   [8, 96, 257]              [8, 32, 257]              3,104\n",
              "├─Head (head_output)                                    [8, 257, 32]              [8, 10]                   --\n",
              "│    └─LayerNorm (layer_norm_prehead)                   [8, 1, 32]                [8, 1, 32]                64\n",
              "│    └─Sequential (head)                                [8, 32, 1]                [8, 10, 1]                --\n",
              "│    │    └─GELU (0)                                    [8, 32, 1]                [8, 32, 1]                --\n",
              "│    │    └─Dropout (1)                                 [8, 32, 1]                [8, 32, 1]                --\n",
              "│    │    └─Conv1d (2)                                  [8, 32, 1]                [8, 10, 1]                330\n",
              "==================================================================================================================================\n",
              "Total params: 51,562\n",
              "Trainable params: 51,562\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 52.44\n",
              "==================================================================================================================================\n",
              "Input size (MB): 0.10\n",
              "Forward/backward pass size (MB): 13.16\n",
              "Params size (MB): 0.11\n",
              "Estimated Total Size (MB): 13.36\n",
              "=================================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT_internet(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        # self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        # self.pool = pool\n",
        "        # self.to_latent = nn.Identity()\n",
        "\n",
        "        # self.mlp_head = nn.Sequential(\n",
        "        #     nn.LayerNorm(dim),\n",
        "        #     nn.Linear(dim, num_classes)\n",
        "        # )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "        print(x.shape)\n",
        "        return\n",
        "\n",
        "        # x = self.transformer(x)\n",
        "\n",
        "        # x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        # x = self.to_latent(x)\n",
        "        # return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "orxDCeaiZp5j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "model_internet = ViT_internet(image_size = 32, patch_size = 2 , num_classes = 10,dim = 32,depth = 4, heads =4, mlp_dim = 10).to(device)\n",
        "summary(model_internet, input_size=(batch_size, 3, 32, 32), col_names = ['input_size','output_size','num_params'],row_settings = [\"var_names\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YavRTOgZtWT",
        "outputId": "1df3a51e-f2db-48e7-9919-8451cf275ea2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 257, 32])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===================================================================================================================\n",
              "Layer (type (var_name))                  Input Shape               Output Shape              Param #\n",
              "===================================================================================================================\n",
              "ViT_internet (ViT_internet)              [8, 3, 32, 32]            --                        8,256\n",
              "├─Sequential (to_patch_embedding)        [8, 3, 32, 32]            [8, 256, 32]              --\n",
              "│    └─Rearrange (0)                     [8, 3, 32, 32]            [8, 256, 12]              --\n",
              "│    └─Linear (1)                        [8, 256, 12]              [8, 256, 32]              416\n",
              "├─Dropout (dropout)                      [8, 257, 32]              [8, 257, 32]              --\n",
              "===================================================================================================================\n",
              "Total params: 8,672\n",
              "Trainable params: 8,672\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.00\n",
              "===================================================================================================================\n",
              "Input size (MB): 0.10\n",
              "Forward/backward pass size (MB): 0.52\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.62\n",
              "==================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ToPatches(nn.Sequential):\n",
        "    def __init__(self, in_channels, channels, patch_size, hidden_channels=32):\n",
        "        super().__init__(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(hidden_channels, channels, patch_size, stride=patch_size)\n",
        "        )"
      ],
      "metadata": {
        "id": "WSZWj61Lh0mO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AddPositionEmbedding(nn.Module):\n",
        "    def __init__(self, channels, shape):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Parameter(torch.Tensor(channels, *shape))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x + self.pos_embedding"
      ],
      "metadata": {
        "id": "hHj4DI78h2mO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ToEmbedding(nn.Sequential):\n",
        "    def __init__(self, in_channels, channels, patch_size, shape, p_drop=0.):\n",
        "        super().__init__(\n",
        "            ToPatches(in_channels, channels, patch_size),\n",
        "            AddPositionEmbedding(channels, shape),\n",
        "            nn.Dropout(p_drop)\n",
        "        )"
      ],
      "metadata": {
        "id": "vYn6V-LShvNN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT_rohan(nn.Sequential):\n",
        "    def __init__(self, classes, image_size, channels, head_channels, num_blocks, patch_size,\n",
        "                 in_channels=3, emb_p_drop=0., trans_p_drop=0., head_p_drop=0.):\n",
        "        reduced_size = image_size // patch_size\n",
        "        shape = (reduced_size, reduced_size)\n",
        "        super().__init__(\n",
        "            ToEmbedding(in_channels, channels, patch_size, shape, emb_p_drop),\n",
        "            # TransformerStack(num_blocks, channels, head_channels, shape, trans_p_drop),\n",
        "            # Head(channels, classes, head_p_drop)\n",
        "        )\n",
        "        # self.reset_parameters()\n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.LayerNorm):\n",
        "                nn.init.constant_(m.weight, 1.)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, AddPositionEmbedding):\n",
        "                nn.init.normal_(m.pos_embedding, mean=0.0, std=0.02)\n",
        "            elif isinstance(m, SelfAttention2d):\n",
        "                nn.init.normal_(m.pos_enc, mean=0.0, std=0.02)\n",
        "            elif isinstance(m, Residual):\n",
        "                nn.init.zeros_(m.gamma)\n",
        "    \n",
        "    def separate_parameters(self):\n",
        "        parameters_decay = set()\n",
        "        parameters_no_decay = set()\n",
        "        modules_weight_decay = (nn.Linear, nn.Conv2d)\n",
        "        modules_no_weight_decay = (nn.LayerNorm,)\n",
        "\n",
        "        for m_name, m in self.named_modules():\n",
        "            for param_name, param in m.named_parameters():\n",
        "                full_param_name = f\"{m_name}.{param_name}\" if m_name else param_name\n",
        "\n",
        "                if isinstance(m, modules_no_weight_decay):\n",
        "                    parameters_no_decay.add(full_param_name)\n",
        "                elif param_name.endswith(\"bias\"):\n",
        "                    parameters_no_decay.add(full_param_name)\n",
        "                elif isinstance(m, Residual) and param_name.endswith(\"gamma\"):\n",
        "                    parameters_no_decay.add(full_param_name)\n",
        "                elif isinstance(m, AddPositionEmbedding) and param_name.endswith(\"pos_embedding\"):\n",
        "                    parameters_no_decay.add(full_param_name)\n",
        "                elif isinstance(m, SelfAttention2d) and param_name.endswith(\"pos_enc\"):\n",
        "                    parameters_no_decay.add(full_param_name)\n",
        "                elif isinstance(m, modules_weight_decay):\n",
        "                    parameters_decay.add(full_param_name)\n",
        "\n",
        "        # sanity check\n",
        "        # assert len(parameters_decay & parameters_no_decay) == 0\n",
        "        # assert len(parameters_decay) + len(parameters_no_decay) == len(list(model.parameters()))\n",
        "\n",
        "        return parameters_decay, parameters_no_decay"
      ],
      "metadata": {
        "id": "VjoAHLDdhpM4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "NUM_CLASSES, IMAGE_SIZE = 10, 32\n",
        "\n",
        "model_rohan = ViT_rohan(NUM_CLASSES, IMAGE_SIZE, channels=32, head_channels=8, num_blocks=4, patch_size=2).to(device)\n",
        "summary(model_rohan, input_size=(batch_size, 3, 32, 32), col_names = ['input_size','output_size','num_params'],row_settings = [\"var_names\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rY11cNnIh782",
        "outputId": "4e2f4dba-8ed6-4e90-820c-e2bde87298af"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===================================================================================================================\n",
              "Layer (type (var_name))                  Input Shape               Output Shape              Param #\n",
              "===================================================================================================================\n",
              "ViT_rohan (ViT_rohan)                    [8, 3, 32, 32]            [8, 32, 16, 16]           --\n",
              "├─ToEmbedding (0)                        [8, 3, 32, 32]            [8, 32, 16, 16]           --\n",
              "│    └─ToPatches (0)                     [8, 3, 32, 32]            [8, 32, 16, 16]           --\n",
              "│    │    └─Conv2d (0)                   [8, 3, 32, 32]            [8, 32, 32, 32]           896\n",
              "│    │    └─GELU (1)                     [8, 32, 32, 32]           [8, 32, 32, 32]           --\n",
              "│    │    └─Conv2d (2)                   [8, 32, 32, 32]           [8, 32, 16, 16]           4,128\n",
              "│    └─AddPositionEmbedding (1)          [8, 32, 16, 16]           [8, 32, 16, 16]           8,192\n",
              "│    └─Dropout (2)                       [8, 32, 16, 16]           [8, 32, 16, 16]           --\n",
              "===================================================================================================================\n",
              "Total params: 13,216\n",
              "Trainable params: 13,216\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 15.79\n",
              "===================================================================================================================\n",
              "Input size (MB): 0.10\n",
              "Forward/backward pass size (MB): 3.15\n",
              "Params size (MB): 0.05\n",
              "Estimated Total Size (MB): 3.30\n",
              "==================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}